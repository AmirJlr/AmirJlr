{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKx7e12tBnpIEueR+luDPV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirJlr/AmirJlr/blob/main/03_prompt_tuning_and_PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Tuning and PEFT\n",
        "\n",
        "**Prompt tuning**, a specific type of **parameter-efficient fine-tuning (PEFT)**, involves adjusting a **pre-trained language model (PLM**) for a specific task without modifying the model's original parameters. Instead, it adds a small set of learnable parameters, called \"soft prompts,\" to the input of the PLM, allowing the model to adapt to the task by learning how to use these soft prompts.\n",
        "\n",
        "\n",
        "## Elaboration:\n",
        "### PEFT (Parameter-Efficient Fine-Tuning):##\n",
        "PEFT is a broader category of techniques that aim to adapt PLMs to specific tasks efficiently. It achieves this by only updating a subset of the model's parameters, typically a small number of new parameters, rather than the entire model.\n",
        "\n",
        "### Prompt Tuning:\n",
        "Within PEFT, prompt tuning focuses on adding learnable \"soft prompts\" to the input of the PLM. These soft prompts are small vectors of trainable parameters that are prepended to the input sequence.\n",
        "\n",
        "\n",
        "### How it works:\n",
        "- The PLM's internal weights are typically kept frozen during prompt tuning.\n",
        "\n",
        "- The soft prompts are trained alongside the input on the target task's data.\n",
        "\n",
        "- By learning the appropriate prompts, the PLM can be guided to generate outputs that are tailored to the specific task.\n",
        "\n",
        "\n",
        "### Benefits of Prompt Tuning:\n",
        "- **Parameter Efficiency**: Only a small subset of parameters is updated, requiring less memory and computational resources.\n",
        "\n",
        "\n",
        "- **Preservation of General Knowledge**: The PLM's original knowledge is preserved because its internal weights are not modified.\n",
        "\n",
        "\n",
        "- **Flexibility**: Prompt tuning allows for rapid adaptation to different tasks by simply changing the soft prompts, making it a suitable choice for dynamic environments.\n",
        "\n",
        "\n",
        "### Comparison with Fine-Tuning:\n",
        "Traditional fine-tuning, on the other hand, involves retraining the PLM's entire set of parameters on a new dataset, which is more resource-intensive and can lead to \"catastrophic forgetting\" where the model loses its pre-trained knowledge.\n",
        "\n",
        "In essence, prompt tuning provides a more efficient and flexible way to adapt PLMs to specific tasks by fine-tuning the input prompts rather than the entire model's architecture."
      ],
      "metadata": {
        "id": "45puyn3PTTl_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-nJOESiTVZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}